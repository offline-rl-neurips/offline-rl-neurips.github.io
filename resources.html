---
layout: default
---

<div class="row">
<p>
This page contains a non-exhaustive list of resources for machine learning and reinforcement learning researchers and practitioners to learn more about offline RL. Please feel free to contact us at <a href="mailto:offline-rl-neurips@google.com">offline-rl-neurips@google.com</a> if you would like to suggest other resources to be added, or <a href="https://github.com/offline-rl-neurips/offline-rl-neurips.github.io/edit/master/resources.html">submit a PR on GitHub</a>.
</p>
</div>

<!-- <div id="tutorials" class="row">
  <h2> Survery, Tutorial and Blog Posts </h2>
  <p> <span class="smallcaps"> Levine S., Kumar A., Tucker G., Fu J.</span> 2020. Offline RL Tutorial, Review and Perspectives on Open Problems.
  <p><span class="smallcaps">Agarwal, R.</span> 2020. An optimistic perspective on offline reinforcement learning..</p>
</div> -->

<div id="references" class="row"> 
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<h2>References (Alphabetical Order)</h2>
<div id="ref-rishabh_blog">
<p><span class="smallcaps">Agarwal, R.</span> 2020. An optimistic perspective on offline reinforcement learning..</p>
</div>
<div id="ref-agarwal2019striving">
<p><span class="smallcaps">Agarwal, R., Schuurmans, D., and Norouzi, M.</span> 2020. An optimistic perspective on offline reinforcement learning. <em>International conference on machine learning</em>.</p>
</div>
<div id="ref-bodnar2019quantile">
<p><span class="smallcaps">Bodnar, C., Li, A., Hausman, K., Pastor, P., and Kalakrishnan, M.</span> 2019. Quantile qt-opt for risk-aware vision-based robotic grasping. <em>arXiv preprint arXiv:1910.02787</em>.</p>
</div>
<div id="ref-bottou2013counterfactual">
<p><span class="smallcaps">Bottou, L., Peters, J., Quiñonero-Candela, J., et al.</span> 2013. Counterfactual reasoning and learning systems: The example of computational advertising. <em>JMLR</em>.</p>
</div>
<div id="ref-boyan199lstdq">
<p><span class="smallcaps">Boyan, J. A.</span> 1999. Least-squares temporal difference learning <em>ICML</em>.</p>
</div>
<div id="ref-cabi2019framework">
<p><span class="smallcaps">Cabi, S., Colmenarejo, S.G., Novikov, A., et al.</span> 2019. A framework for data-driven robotics. <em>arXiv preprint arXiv:1909.12200</em>.</p>
</div>
<div id="ref-chen2019information">
<p><span class="smallcaps">Chen, J. and Jiang, N.</span> 2019. Information-theoretic considerations in batch reinforcement learning. <em>ICML</em>.</p>
</div>
<div id="ref-chen2019bail">
<p><span class="smallcaps">Chen, X., Zhou, Z., Wang, Z., et al.</span> 2019. BAIL: Best-action imitation learning for batch deep reinforcement learning. <em>arXiv preprint arXiv:1910.12179</em>.</p>
</div>
<div id="ref-dai2017boosting">
<p><span class="smallcaps">Dai, B., Shaw, A., He, N., Li, L., and Song, L.</span> 2017. Boosting the actor with dual critic. <em>arXiv preprint arXiv:1712.10282</em>.</p>
</div>
<div id="ref-dai2018sbeed">
<p><span class="smallcaps">Dai, B., Shaw, A., Li, L., et al.</span> 2018. SBEED: Convergent reinforcement learning with nonlinear function approximation. <em>International conference on machine learning</em>, 1125–1134.</p>
</div>
<div id="ref-degris2012offpac">
  <p><span class="smallcaps">Degris, T., White, M., Sutton, R. S.</span> 2012. Off-policy Actor-Critic. arXiv preprint arXiv:1205.4839. </p>
</div>
<p><span class="smallcaps">Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.</span> 2009. ImageNet: A Large-Scale Hierarchical Image Database. <em>CVPR</em>.</p>
</div>
<div id="ref-imagenet_cvpr09">
<p><span class="smallcaps">Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.</span> 2009. ImageNet: A Large-Scale Hierarchical Image Database. <em>CVPR</em>.</p>
</div>
<div id="ref-dudik2014doubly">
<p><span class="smallcaps">Dudı́k, M., Erhan, D., Langford, J., Li, L., and others</span>. 2014. Doubly robust policy evaluation and optimization. <em>Statistical Science</em> <em>29</em>, 4, 485–511.</p>
</div>
<div id="ref-dulac2019challenges">
<p><span class="smallcaps">Dulac-Arnold, G., Mankowitz, D., and Hester, T.</span> 2019. Challenges of real-world reinforcement learning. <em>arXiv preprint arXiv:1904.12901</em>.</p>
</div>
<div id="ref-ernst2005tree">
<p><span class="smallcaps">Ernst, D., Geurts, P., and Wehenkel, L.</span> 2005. Tree-based batch mode reinforcement learning. <em>JMLR</em>.</p>
</div>
<div id="ref-d4rl">
<p><span class="smallcaps">Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.</span> 2020. D4RL: Datasets for deep data-driven reinforcement learning. <em>ArXiv</em>.</p>
</div>
<div id="ref-farahmand2010error">
<p><span class="smallcaps">Farahmand, A. M., Szepesvári, C., Munos R.</span> 2010. Error propagation for approximate policy and value iteration  <em>NeurIPS</em>.</p>
</div>
<div id="ref-farahmand2011model">
<p><span class="smallcaps">Farahmand, A. M., Szepesvári, C.</span> 2011. Model selection in reinforcement learning. <em>Machine learning,</em> 85(3) 299-332.</p>
</div>
<div id="ref-fujimoto2019benchmarking">
<p><span class="smallcaps">Fujimoto, S., Conti, E., Ghavamzadeh, M., and Pineau, J.</span> 2019. Benchmarking batch deep reinforcement learning algorithms. <em>arXiv preprint arXiv:1910.01708</em>.</p>
</div>
<div id="ref-fujimoto2018off">
<p><span class="smallcaps">Fujimoto, S., Meger, D., and Precup, D.</span> 2018. Off-policy deep reinforcement learning without exploration. <em>arXiv preprint arXiv:1812.02900</em>.</p>
</div>
<div id="ref-gottesman2020interpretable">
<p><span class="smallcaps">Gottesman, O., Futoma, J., Liu, Y., et al.</span> 2020. Interpretable off-policy evaluation in reinforcement learning by highlighting influential transitions. <em>arXiv preprint arXiv:2002.03478</em>.</p>
</div>
<div id="ref-gulcehre2020rl">
<p><span class="smallcaps">Gulcehre, C., Wang, Z., Novikov, A., et al.</span> 2020. RL unplugged: Benchmarks for offline reinforcement learning. <em>arXiv preprint arXiv:2006.13888</em>.</p>
</div>
<div id="ref-hoppe2019qgraph">
<p><span class="smallcaps">Hoppe, S. and Toussaint, M.</span> 2019. Qgraph-bounded q-learning: Stabilizing model-free off-policy deep reinforcement learning..</p>
</div>
<div id="ref-jaques2019way">
<p><span class="smallcaps">Jaques, N., Ghandeharioun, A., Shen, J.H., et al.</span> 2019. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. <em>arXiv preprint arXiv:1907.00456</em>.</p>
</div>
<div id="ref-jiang2016doubly">
<p><span class="smallcaps">Jiang, N. and Li, L.</span> 2016. Doubly robust off-policy value evaluation for reinforcement learning. <em>International conference on machine learning</em>, 652–661.</p>
</div>
<div id="ref-karampatziakis2019empirical">
<p><span class="smallcaps">Karampatziakis, N., Langford, J., and Mineiro, P.</span> 2019. Empirical likelihood for contextual bandits. <em>arXiv preprint arXiv:1906.03323</em>.</p>
</div>
<div id="ref-kidambi2020morel">
<p><span class="smallcaps">Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.</span> 2020. MOReL: Model-based offline reinforcement learning. <em>arXiv preprint arXiv:2005.05951</em>.</p>
</div>
<div id="ref-kumar_blog">
<p><span class="smallcaps">Kumar, A.</span> 2019. Data-driven deep reinforcement learning..</p>
</div>
<div id="ref-kumar2019stabilizing">
<p><span class="smallcaps">Kumar, A., Fu, J., Tucker, G., and Levine, S.</span> 2019. Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. <em>NeurIPS</em>.</p>
</div>
<div id="ref-kumar2020conservative">
<p><span class="smallcaps">Kumar, A., Zhou, A., Tucker, G., and Levine, S.</span> 2020. Conservative q-learning for offline reinforcement learning. <em>arXiv preprint arXiv:2006.04779</em>.</p>
</div>
<div id="ref-lagoudakis2003least">
  <p><span class="smallcaps">Lagoudakis, M. G, Parr, R.</span> 2003. Least-squares policy iteration. <em>JMLR</em>.</p>
</div>
<div id="ref-lange2012batch">
<p><span class="smallcaps">Lange, S., Gabel, T., and Riedmiller, M.</span> 2012. Batch reinforcement learning. <em>Reinforcement learning</em>.</p>
</div>
<div id="ref-langford_talk">
<p><span class="smallcaps">Langford, J.</span> 2019. A real-world reinforcement learning revolution..</p>
</div>
<div id="ref-laroche2019safe">
<p><span class="smallcaps">Laroche, R., Trichelair, P., and Des Combes, R.T.</span> 2019. Safe policy improvement with baseline bootstrapping. <em>International conference on machine learning</em>, 3652–3661.</p>
</div>
<div id="ref-levine2020offline">
<p><span class="smallcaps">Levine, S., Kumar, A., Tucker, G., and Fu, J.</span> 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. <em>arXiv preprint arXiv:2005.01643</em>.</p>
</div>
<div id="ref-liu2019understanding">
<p><span class="smallcaps">Liu, Y., Bacon, P.-L., and Brunskill, E.</span> 2019a. Understanding the curse of horizon in off-policy evaluation via conditional importance sampling. <em>arXiv preprint arXiv:1910.06508</em>.</p>
</div>
<div id="ref-liu2019off">
<p><span class="smallcaps">Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.</span> 2019b. Off-policy policy gradient with state distribution correction. <em>arXiv preprint arXiv:1904.08473</em>.</p>
</div>
<div id="ref-matsushima2020deployment">
<p><span class="smallcaps">Matsushima, T., Furuta, H., Matsuo, Y., Nachum, O., and Gu, S.</span> 2020. Deployment-efficient reinforcement learning via model-based offline optimization. <em>arXiv preprint arXiv:2006.03647</em>.</p>
</div>
<div id="ref-nachum2019dualdice">
<p><span class="smallcaps">Nachum, O., Chow, Y., Dai, B., and Li, L.</span> 2019a. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. <em>Advances in neural information processing systems</em>, 2318–2328.</p>
</div>
<div id="ref-nachum2019algaedice">
<p><span class="smallcaps">Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D.</span> 2019b. AlgaeDICE: Policy gradient from arbitrary experience. <em>arXiv preprint arXiv:1912.02074</em>.</p>
</div>
<div id="ref-nair2020accelerating">
<p><span class="smallcaps">Nair, A., Dalal, M., Gupta, A., and Levine, S.</span> 2020. Accelerating online reinforcement learning with offline datasets. <em>arXiv preprint arXiv:2006.09359</em>.</p>
</div>
<div id="ref-namkoong2020off">
<p><span class="smallcaps">Namkoong, H., Keramati, R., Yadlowsky, S., and Brunskill, E.</span> 2020. Off-policy policy evaluation for sequential decisions under unobserved confounding. <em>arXiv preprint arXiv:2003.05623</em>.</p>
</div>
<div id="ref-peng2019advantage">
<p><span class="smallcaps">Peng, X.B., Kumar, A., Zhang, G., and Levine, S.</span> 2019. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. <em>arXiv preprint arXiv:1910.00177</em>.</p>
</div>
<div id="ref-peshkin2002learning">
<p><span class="smallcaps">Peshkin, L., Shelton, C. R.</span> 2002. Learning from scarce experience. <em>arXiv preprint cs/0204043.</em>.</p>
</div>
<div id="ref-prasad2020defining">
<p><span class="smallcaps">Prasad, N., Engelhardt, B., and Doshi-Velez, F.</span> 2020. Defining admissible rewards for high-confidence policy evaluation in batch reinforcement learning. <em>Proceedings of the acm conference on health, inference, and learning</em>, 1–9.</p>
</div>
<div id="ref-precup2000eligibility">
<p><span class="smallcaps">Precup, D.</span> 2000. Eligibility traces for off-policy policy evaluation. <em>Computer Science Department Faculty Publication Series</em>, 80.</p>
</div>
<div id="ref-precup2001off">
<p><span class="smallcaps">Precup, D., Sutton, R.S., and Dasgupta, S.</span> 2001. Off-policy temporal-difference learning with function approximation. <em>ICML</em>, 417–424.</p>
</div>
<div id="ref-shortreed2011informing">
<p><span class="smallcaps">Shortreed, S.M., Laber, E., Lizotte, D.J., Stroup, T.S., Pineau, J., and Murphy, S.A.</span> 2011. Informing sequential clinical decision-making through reinforcement learning: An empirical study. <em>Machine learning</em>.</p>
</div>
<div id="ref-siegel2020keep">
<p><span class="smallcaps">Siegel, N., Springenberg, J.T., Berkenkamp, F., et al.</span> 2020. Keep doing what worked: Behavior modelling priors for offline reinforcement learning. <em>ICLR</em>.</p>
</div>
<div id="ref-Sohn2020BRPOBR">
<p><span class="smallcaps">Sohn, S., Chow, Y., Ooi, J., et al.</span> 2020. BRPO: Batch residual policy optimization. <em>arXiv:2002.05522</em>.</p>
</div>
<div id="ref-sussexstitched">
<p><span class="smallcaps">Sussex, S., Gottesman, O., Liu, Y., Murphy, S., Brunskill, E., and Doshi-Velez, F.</span> Stitched trajectories for off-policy learning..</p>
</div>
<div id="ref-sutton2009fastgtd">
<p><span class="smallcaps">Sutton, R.S., Maei, H.R., Precup, D., et al.</span> 2009. Fast gradient-descent methods for temporal-difference learning with linear function approximation. <em>Proceedings of the 26th annual international conference on machine learning</em>, 993–1000.</p>
</div>
<div id="ref-sutton1991dyna">
<p><span class="smallcaps">Sutton, R.S.</span> 1991. Dyna, an integrated architecture for learning, planning, and reacting. <em>ACM Sigart Bulletin,</em>, 160-163.</p>
</div>
<div id="ref-thomas2015high">
<p><span class="smallcaps">Thomas, P. S., Theocharous, G., Ghavamzadeh, M. </span> 2015. High-confidence off-policy evaluation. <em>AAAI</em>.</p>
</div>
<div id="ref-wang2020critic">
<p><span class="smallcaps">Wang, Z., Novikov, A., Żołna, K., et al.</span> 2020. Critic Regularized Regression. <em>arXiv e-prints</em>, arXiv:2006.15134.</p>
</div>
<div id="ref-wu2019behavior">
<p><span class="smallcaps">Wu, Y., Tucker, G., and Nachum, O.</span> 2019. Behavior regularized offline reinforcement learning. <em>arXiv preprint arXiv:1911.11361</em>.</p>
</div>
<div id="ref-Xie2020QAS">
<p><span class="smallcaps">Xie, T. and Jiang, N.</span> 2020. Q* approximation schemes for batch reinforcement learning: A theoretical comparison. <em>ArXiv</em> <em>abs/2003.03924</em>.</p>
</div>
<div id="ref-yu2020mopo">
<p><span class="smallcaps">Yu, T., Thomas, G., Yu, L., et al.</span> 2020. MOPO: Model-based offline policy optimization. <em>arXiv preprint arXiv:2005.13239</em>.</p>
</div>
</div>
